{
 "metadata": {
  "name": "",
  "signature": "sha256:684c0db7244920076a9c0cdf80422abd1baef74c2f058c0d8a987c1284e7a987"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Natural Language Processing (NLP)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 1: Introduction\n",
      "\n",
      "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What is NLP?\n",
      "\n",
      "- Using computers to process (analyze, understand, generate) natural human languages"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Why is NLP useful?\n",
      "\n",
      "- Most knowledge created by humans is unstructured text\n",
      "- Need some way to make sense of it\n",
      "- Enables quantitative analysis of text data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What are some of the higher level task areas?\n",
      "\n",
      "- **Speech recognition and generation**: Apple Siri\n",
      "    - Speech to text\n",
      "    - Text to speech\n",
      "- **Question answering**: IBM Watson\n",
      "    - Match query with knowledge base\n",
      "    - Reasoning about intent of question\n",
      "- **Machine translation**: Google Translate\n",
      "    - One language to another to another\n",
      "- **Information retrieval**: Google\n",
      "    - Finding relevant results\n",
      "    - Finding similar results\n",
      "- **Information extraction**: Gmail\n",
      "    - Structured information from unstructured documents\n",
      "- **Assistive technologies**: Google autocompletion\n",
      "    - Predictive text input\n",
      "    - Text simplification\n",
      "- **Natural Language Generation**: computer-generated articles\n",
      "    - Generating text from data\n",
      "- **Automatic summarization**: Google News\n",
      "    - Extractive summarization\n",
      "    - Abstractive summarization\n",
      "- **Sentiment analysis**: Twitter analysis\n",
      "    - Attitude of speaker"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### What are some of the lower level components?\n",
      "\n",
      "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
      "- **Stopword removal**: a/an/the\n",
      "- **Stemming and lemmatization**: root word\n",
      "- **TF-IDF**: word importance\n",
      "- **Part-of-speech tagging**: noun/verb/adjective\n",
      "- **Named entity recognition**: person/organization/location\n",
      "- **Spelling correction**: \"New Yrok City\"\n",
      "- **Word sense disambiguation**: \"buy a mouse\"\n",
      "- **Segmentation**: \"New York City subway\"\n",
      "- **Language detection**: \"translate this page\"\n",
      "- **Machine learning**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Why is NLP hard?\n",
      "\n",
      "- **Ambiguity**:\n",
      "    - Teacher Strikes Idle Kids\n",
      "    - Red Tape Holds Up New Bridges\n",
      "    - Hospitals are Sued by 7 Foot Doctors\n",
      "    - Juvenile Court to Try Shooting Defendant\n",
      "    - Local High School Dropouts Cut in Half\n",
      "- **Non-standard English**: tweets/text messages\n",
      "- **Idioms**: \"throw in the towel\"\n",
      "- **Newly coined words**: \"retweet\"\n",
      "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
      "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### How does NLP work?\n",
      "\n",
      "- Build probabilistic model using data about a language\n",
      "- Requires an understanding of the language\n",
      "- Requires an understanding of the world"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Reading in the Yelp Reviews"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- \"corpus\" = collection of documents\n",
      "- \"corpora\" = plural form of corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn import metrics\n",
      "from textblob import TextBlob, Word\n",
      "from nltk.stem.snowball import SnowballStemmer\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read yelp.csv into a DataFrame\n",
      "url = 'https://raw.githubusercontent.com/justmarkham/DAT7/master/data/yelp.csv'\n",
      "yelp = pd.read_csv(url)\n",
      "\n",
      "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
      "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
      "\n",
      "# split the new DataFrame into training and testing sets\n",
      "X_train, X_test, y_train, y_test = train_test_split(yelp_best_worst.text, yelp_best_worst.stars, random_state=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 3: Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **What:** Separate text into units such as sentences or words\n",
      "- **Why:** Gives structure to previously unstructured text\n",
      "- **Notes:** Relatively easy with English language text, not easy with some languages"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
      "vect = CountVectorizer()\n",
      "train_dtm = vect.fit_transform(X_train)\n",
      "test_dtm = vect.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rows are documents, columns are terms (aka \"tokens\" or \"features\")\n",
      "train_dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# last 50 features\n",
      "print vect.get_feature_names()[-50:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show vectorizer options\n",
      "vect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **lowercase:** boolean, True by default\n",
      "- Convert all characters to lowercase before tokenizing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# don't convert to lowercase\n",
      "vect = CountVectorizer(lowercase=False)\n",
      "train_dtm = vect.fit_transform(X_train)\n",
      "train_dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **token_pattern:** string\n",
      "- Regular expression denoting what constitutes a \"token\". The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# allow tokens of one character\n",
      "vect = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
      "train_dtm = vect.fit_transform(X_train)\n",
      "train_dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **ngram_range:** tuple (min_n, max_n)\n",
      "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# include 1-grams and 2-grams\n",
      "vect = CountVectorizer(ngram_range=(1, 2))\n",
      "train_dtm = vect.fit_transform(X_train)\n",
      "train_dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# last 50 features\n",
      "print vect.get_feature_names()[-50:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Predicting the star rating:**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use default options for CountVectorizer\n",
      "vect = CountVectorizer()\n",
      "\n",
      "# create document-term matrices\n",
      "train_dtm = vect.fit_transform(X_train)\n",
      "test_dtm = vect.transform(X_test)\n",
      "\n",
      "# use Naive Bayes to predict the star rating\n",
      "nb = MultinomialNB()\n",
      "nb.fit(train_dtm, y_train)\n",
      "y_pred_class = nb.predict(test_dtm)\n",
      "\n",
      "# calculate accuracy\n",
      "print metrics.accuracy_score(y_test, y_pred_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate null accuracy\n",
      "y_test_binary = np.where(y_test==5, 1, 0)\n",
      "y_test_binary.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function that accepts a vectorizer and returns the accuracy\n",
      "def tokenize_test(vect):\n",
      "    train_dtm = vect.fit_transform(X_train)\n",
      "    print 'Features: ', train_dtm.shape[1]\n",
      "    test_dtm = vect.transform(X_test)\n",
      "    nb = MultinomialNB()\n",
      "    nb.fit(train_dtm, y_train)\n",
      "    y_pred_class = nb.predict(test_dtm)\n",
      "    print 'Accuracy: ', metrics.accuracy_score(y_test, y_pred_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# include 1-grams and 2-grams\n",
      "vect = CountVectorizer(ngram_range=(1, 2))\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 4: Stopword Removal"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **What:** Remove common words that will likely appear in any text\n",
      "- **Why:** They don't tell you much about your text"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show vectorizer options\n",
      "vect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **stop_words:** string {'english'}, list, or None (default)\n",
      "- If 'english', a built-in stop word list for English is used.\n",
      "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
      "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove English stop words\n",
      "vect = CountVectorizer(stop_words='english')\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set of stop words\n",
      "print vect.get_stop_words()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 5: Other CountVectorizer Options"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **max_features:** int or None, default=None\n",
      "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove English stop words and only keep 100 features\n",
      "vect = CountVectorizer(stop_words='english', max_features=100)\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# all 100 features\n",
      "print vect.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# include 1-grams and 2-grams, and limit the number of features\n",
      "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
      "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
      "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 6: Introduction to TextBlob"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TextBlob: \"Simplified Text Processing\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the first review\n",
      "print yelp_best_worst.text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save it as a TextBlob object\n",
      "review = TextBlob(yelp_best_worst.text[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list the words\n",
      "review.words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# list the sentences\n",
      "review.sentences"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some string methods are available\n",
      "review.lower()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 7: Stemming and Lemmatization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Stemming:**\n",
      "\n",
      "- **What:** Reduce a word to its base/stem/root form\n",
      "- **Why:** Often makes sense to treat related words the same way\n",
      "- **Notes:**\n",
      "    - Uses a \"simple\" and fast rule-based approach\n",
      "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
      "    - Some search engines treat words with the same stem as synonyms"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize stemmer\n",
      "stemmer = SnowballStemmer('english')\n",
      "\n",
      "# stem each word\n",
      "print [stemmer.stem(word) for word in review.words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Lemmatization**\n",
      "\n",
      "- **What:** Derive the canonical form ('lemma') of a word\n",
      "- **Why:** Can be better than stemming\n",
      "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assume every word is a noun\n",
      "print [word.lemmatize() for word in review.words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# assume every word is a verb\n",
      "print [word.lemmatize(pos='v') for word in review.words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function that accepts text and returns a list of lemmas\n",
      "def split_into_lemmas(text):\n",
      "    text = unicode(text, 'utf-8').lower()\n",
      "    words = TextBlob(text).words\n",
      "    return [word.lemmatize() for word in words]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use split_into_lemmas as the feature extraction function\n",
      "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
      "tokenize_test(vect)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# last 50 features\n",
      "print vect.get_feature_names()[-50:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 8: Term Frequency - Inverse Document Frequency (TF-IDF)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
      "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
      "- **Notes:** Used for search engine scoring, text summarization, document clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example documents\n",
      "train_simple = ['call you tonight',\n",
      "                'Call me a cab',\n",
      "                'please call me... PLEASE!']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# CountVectorizer\n",
      "vect = CountVectorizer()\n",
      "pd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TfidfVectorizer\n",
      "vect = TfidfVectorizer()\n",
      "pd.DataFrame(vect.fit_transform(train_simple).toarray(), columns=vect.get_feature_names())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 9: Using TF-IDF to Summarize a Yelp Review"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a document-term matrix using TF-IDF\n",
      "vect = TfidfVectorizer(stop_words='english')\n",
      "dtm = vect.fit_transform(yelp.text)\n",
      "features = vect.get_feature_names()\n",
      "dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def summarize():\n",
      "    \n",
      "    # choose a random review\n",
      "    review_id = np.random.randint(0, len(yelp))\n",
      "    review_text = unicode(yelp.text[review_id], 'utf-8')\n",
      "    \n",
      "    # create a dictionary of words and their TF-IDF scores\n",
      "    word_scores = {}\n",
      "    for word in TextBlob(review_text).words:\n",
      "        word = word.lower()\n",
      "        if word in features:\n",
      "            word_scores[word] = dtm[review_id, features.index(word)]\n",
      "    \n",
      "    # print words with the top 5 TF-IDF scores\n",
      "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
      "    for word, score in top_scores:\n",
      "        print word\n",
      "    \n",
      "    # print the review\n",
      "    print '\\n' + review_text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summarize()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 10: Sentiment Analysis"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print review"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
      "review.sentiment.polarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# understanding the apply method\n",
      "yelp['length'] = yelp.text.apply(len)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function that accepts text and returns the polarity\n",
      "def detect_sentiment(text):\n",
      "    return TextBlob(text.decode('utf-8')).sentiment.polarity"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a new DataFrame column for sentiment\n",
      "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# boxplot of sentiment grouped by stars\n",
      "yelp.boxplot(column='sentiment', by='stars')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reviews with most positive sentiment\n",
      "yelp[yelp.sentiment == 1].text.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reviews with most negative sentiment\n",
      "yelp[yelp.sentiment == -1].text.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# widen the column display\n",
      "pd.set_option('max_colwidth', 500)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# negative sentiment in a 5-star review\n",
      "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# positive sentiment in a 1-star review\n",
      "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reset the column display width\n",
      "pd.reset_option('max_colwidth')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 11: Adding Features to a Document-Term Matrix"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
      "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
      "\n",
      "# split the new DataFrame into training and testing sets\n",
      "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
      "X = yelp_best_worst[feature_cols]\n",
      "y = yelp_best_worst.stars\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use CountVectorizer with text column only\n",
      "vect = CountVectorizer()\n",
      "train_dtm = vect.fit_transform(X_train[:, 0])\n",
      "test_dtm = vect.transform(X_test[:, 0])\n",
      "print train_dtm.shape\n",
      "print test_dtm.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# shape of other four feature columns\n",
      "X_train[:, 1:].shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# cast other feature columns to float and convert to a sparse matrix\n",
      "extra = sp.sparse.csr_matrix(X_train[:, 1:].astype(float))\n",
      "extra.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# combine sparse matrices\n",
      "train_dtm_extra = sp.sparse.hstack((train_dtm, extra))\n",
      "train_dtm_extra.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# repeat for testing set\n",
      "extra = sp.sparse.csr_matrix(X_test[:, 1:].astype(float))\n",
      "test_dtm_extra = sp.sparse.hstack((test_dtm, extra))\n",
      "test_dtm_extra.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use logistic regression with text column only\n",
      "logreg = LogisticRegression(C=1e9)\n",
      "logreg.fit(train_dtm, y_train)\n",
      "y_pred_class = logreg.predict(test_dtm)\n",
      "print metrics.accuracy_score(y_test, y_pred_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# use logistic regression with all features\n",
      "logreg = LogisticRegression(C=1e9)\n",
      "logreg.fit(train_dtm_extra, y_train)\n",
      "y_pred_class = logreg.predict(test_dtm_extra)\n",
      "print metrics.accuracy_score(y_test, y_pred_class)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 12: Fun TextBlob Features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# spelling correction\n",
      "TextBlob('15 minuets late').correct()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# spellcheck\n",
      "Word('parot').spellcheck()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# definitions\n",
      "Word('bank').define('v')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# language identification\n",
      "TextBlob('Hola amigos').detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}